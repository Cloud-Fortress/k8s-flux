# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  # Set the default image and provider to use
  default_image = "rockylinux/9"
  default_version = "1.0.0"
  default_provider = "virtualbox"
  # Choose a different image and provider for macOS
  if RUBY_PLATFORM =~ /darwin/
    default_image = "marcinbojko/rockylinux9_arm64"
    default_version = "9.1.1"
    default_provider = "parallels"
  end
  # Choose a different image and provider for Linux
  if RUBY_PLATFORM =~ /linux/
    default_provider = "libvirt"
  end
  # Configure the first node as the control plane
  config.vm.boot_timeout = 600
  config.ssh.private_key_path = ['~/.vagrant.d/insecure_private_key', '~/.ssh/id_rsa']
  config.ssh.forward_agent = true
  config.vm.define "control-plane" do |node|
    node.vm.box = default_image
    node.vm.box_version = default_version
    node.vm.hostname = "control-plane.local"
    node.vm.network "private_network", ip: "192.168.50.10"
    node.vm.provider default_provider do |vb|
      vb.name = "control-plane"
      vb.memory = "2048"
      vb.cpus = "2"
    end
    
    # Install Ansible and Git
    node.vm.provision "shell", inline: <<-SHELL
      sudo dnf update -y
      sudo dnf install -y ansible git
      sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config
      echo 'root:vagrant' | chpasswd
      systemctl restart sshd
    SHELL
    
    # Clone the k8s-ansible repository and execute run.sh
    node.vm.provision "shell", inline: <<-SHELL
      sudo -i -u vagrant bash -c "git clone git@github.com:Cloud-Fortress/k8s-ansible.git"
      cd k8s-ansible
      ./run.sh -i inventory/staging.yml playbooks/kube.yml
    SHELL
    
    # Initialize the control plane and generate a token
    node.vm.provision "shell", inline: <<-SHELL
      sudo kubeadm init --apiserver-advertise-address=192.168.50.10 --pod-network-cidr=10.244.0.0/16
      mkdir -p /home/vagrant/.kube
      sudo cp -i /etc/kubernetes/admin.conf /home/vagrant/.kube/config
      sudo chown $(id -u vagrant):$(id -g vagrant) /home/vagrant/.kube/config
      kubeadm token create --print-join-command > /tmp/join-command.sh
      chmod +x /tmp/join-command.sh
    SHELL
  end
  
  # Configure the second node as a worker node
  config.vm.define "worker" do |node|
    node.vm.box = default_image
    node.vm.box_version = default_version
    node.vm.hostname = "worker.local"
    node.vm.network "private_network", ip: "192.168.50.11"
    node.vm.provider default_provider do |vb|
      vb.name = "worker"
      vb.memory = "2048"
      vb.cpus = "2"
    end
    
    # Join the worker node to the cluster using the token generated by the control-plane node
    node.vm.provision "shell", inline: <<-SHELL
      sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config
      echo 'root:vagrant' | chpasswd
      systemctl restart sshd
    SHELL
  end

  # Configure the third node as a nas node
  config.vm.define "nas" do |node|
    node.vm.box = default_image
    node.vm.box_version = default_version
    node.vm.hostname = "nas.local"
    node.vm.network "private_network", ip: "192.168.50.11"
    node.vm.provider default_provider do |vb|
      vb.name = "nas"
      vb.memory = "2048"
      vb.cpus = "2"
    end
    
    # Join the nas node to the cluster using the token generated by the control-plane node
    node.vm.provision "shell", inline: <<-SHELL
      sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config
      echo 'root:vagrant' | chpasswd
      systemctl restart sshd
    SHELL
  end
end